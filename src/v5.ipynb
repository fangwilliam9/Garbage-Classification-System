{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f536fb-2964-4f8b-ba94-ee3fe7261fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2527 files belonging to 6 classes.\n",
      "Using 2022 files for training.\n",
      "Found 2527 files belonging to 6 classes.\n",
      "Using 505 files for validation.\n",
      "类别索引映射: ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
      "Epoch 1/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 713ms/step - accuracy: 0.2838 - loss: 2.0990 - val_accuracy: 0.6891 - val_loss: 1.1562\n",
      "Epoch 2/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 679ms/step - accuracy: 0.5606 - loss: 1.3517 - val_accuracy: 0.7604 - val_loss: 0.9184\n",
      "Epoch 3/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 677ms/step - accuracy: 0.6395 - loss: 1.1917 - val_accuracy: 0.7842 - val_loss: 0.8371\n",
      "Epoch 4/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 678ms/step - accuracy: 0.6582 - loss: 1.1190 - val_accuracy: 0.7921 - val_loss: 0.8350\n",
      "Epoch 5/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 688ms/step - accuracy: 0.6878 - loss: 1.0839 - val_accuracy: 0.7980 - val_loss: 0.8250\n",
      "Epoch 6/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 691ms/step - accuracy: 0.6834 - loss: 1.0376 - val_accuracy: 0.7980 - val_loss: 0.7932\n",
      "Epoch 7/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 678ms/step - accuracy: 0.7255 - loss: 0.9559 - val_accuracy: 0.7941 - val_loss: 0.7565\n",
      "Epoch 8/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 681ms/step - accuracy: 0.6927 - loss: 0.9756 - val_accuracy: 0.8178 - val_loss: 0.7498\n",
      "Epoch 9/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 693ms/step - accuracy: 0.7311 - loss: 0.9445 - val_accuracy: 0.8277 - val_loss: 0.7166\n",
      "Epoch 10/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 679ms/step - accuracy: 0.7373 - loss: 0.9175 - val_accuracy: 0.8337 - val_loss: 0.6911\n",
      "Epoch 11/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 678ms/step - accuracy: 0.7500 - loss: 0.8714 - val_accuracy: 0.8059 - val_loss: 0.7194\n",
      "Epoch 12/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 680ms/step - accuracy: 0.7540 - loss: 0.9530 - val_accuracy: 0.8297 - val_loss: 0.6889\n",
      "Epoch 13/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 681ms/step - accuracy: 0.7625 - loss: 0.8070 - val_accuracy: 0.8515 - val_loss: 0.6566\n",
      "Epoch 14/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 682ms/step - accuracy: 0.7511 - loss: 0.8931 - val_accuracy: 0.8396 - val_loss: 0.6653\n",
      "Epoch 15/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 682ms/step - accuracy: 0.7437 - loss: 0.8772 - val_accuracy: 0.8277 - val_loss: 0.6842\n",
      "Epoch 16/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 679ms/step - accuracy: 0.7663 - loss: 0.8267 - val_accuracy: 0.8317 - val_loss: 0.6690\n",
      "Epoch 17/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 678ms/step - accuracy: 0.7683 - loss: 0.8055 - val_accuracy: 0.8257 - val_loss: 0.6513\n",
      "Epoch 18/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 679ms/step - accuracy: 0.7852 - loss: 0.7818 - val_accuracy: 0.8455 - val_loss: 0.6303\n",
      "Epoch 19/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 687ms/step - accuracy: 0.7852 - loss: 0.7729 - val_accuracy: 0.8238 - val_loss: 0.6656\n",
      "Epoch 20/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 678ms/step - accuracy: 0.7749 - loss: 0.8075 - val_accuracy: 0.8356 - val_loss: 0.6612\n",
      "Epoch 1/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 973ms/step - accuracy: 0.6666 - loss: 1.1113 - val_accuracy: 0.8396 - val_loss: 0.6189\n",
      "Epoch 2/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 946ms/step - accuracy: 0.7180 - loss: 0.9413 - val_accuracy: 0.8376 - val_loss: 0.6177\n",
      "Epoch 3/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 944ms/step - accuracy: 0.7306 - loss: 0.9401 - val_accuracy: 0.8396 - val_loss: 0.6153\n",
      "Epoch 4/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 943ms/step - accuracy: 0.7323 - loss: 0.9016 - val_accuracy: 0.8396 - val_loss: 0.6152\n",
      "Epoch 5/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 946ms/step - accuracy: 0.7612 - loss: 0.8633 - val_accuracy: 0.8376 - val_loss: 0.6168\n",
      "Epoch 6/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 943ms/step - accuracy: 0.7560 - loss: 0.8322 - val_accuracy: 0.8376 - val_loss: 0.6202\n",
      "Epoch 7/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 944ms/step - accuracy: 0.7674 - loss: 0.8321 - val_accuracy: 0.8455 - val_loss: 0.6180\n",
      "Epoch 8/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 941ms/step - accuracy: 0.7828 - loss: 0.7860 - val_accuracy: 0.8455 - val_loss: 0.6129\n",
      "Epoch 9/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 942ms/step - accuracy: 0.7780 - loss: 0.8089 - val_accuracy: 0.8396 - val_loss: 0.6124\n",
      "Epoch 10/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 943ms/step - accuracy: 0.7965 - loss: 0.7596 - val_accuracy: 0.8376 - val_loss: 0.6144\n",
      "模型已保存！\n"
     ]
    }
   ],
   "source": [
    "# train_model.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# 数据加载\n",
    "batch_size = 32\n",
    "img_size = (224, 224)\n",
    "dataset_path = \"../dataset/Garbage classification/Garbage classification\"\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_dataset = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "print(f\"类别索引映射: {class_names}\")\n",
    "\n",
    "# 数据增强\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.4),\n",
    "    layers.RandomZoom(0.4),\n",
    "    layers.RandomContrast(0.3),\n",
    "    layers.RandomBrightness(0.2),\n",
    "    layers.RandomTranslation(0.2, 0.2),  # 平移增强\n",
    "    layers.RandomShear(0.2)  # 形变增强\n",
    "])\n",
    "\n",
    "# 加载预训练模型\n",
    "base_model = keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "base_model.trainable = False  # 先冻结\n",
    "\n",
    "# 构建模型\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# 计算类别权重\n",
    "y_train = []\n",
    "for images, labels in train_dataset:\n",
    "    y_train.extend(labels.numpy())\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# 编译模型\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# 早停策略\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "epochs = 20\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    class_weight=class_weights,  # 这里才是正确的\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# 解冻部分层进行微调\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:100]:   # 让前100层冻结\n",
    "    layer.trainable = False\n",
    "\n",
    "# 重新编译模型\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# 继续训练（微调）\n",
    "fine_tune_epochs = 10\n",
    "history_fine = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=fine_tune_epochs,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"classification_model.keras\")\n",
    "print(\"模型已保存！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
